  1：条件寄存器的作用：会保存最近执行的算术或逻辑指令的信息，例如if和while的状态
  
  2：纠正自己一个错误的认知，指针变量的大小取决于计算机位数，比如64位计算机，指针变量的大小是8字节，这个规则在任何情况下都适用，比如char * b[n]的空间不是n*1=n个字节，而是n个char * ，即n*8个字节。

  3：提醒自己一条，QT的使用也遵循cpp的基本原则，即如果是int * pValue=nValue; pValue+2必定得到的是地址值，而*pValue+2才是真正想要的结果，即使QT写多了，也不要忘记这种基础的使用方法。

  4:奇偶校验位在整数运算时大部分情况下是不设置的，但是对于浮点数的操作，每一次运算都会设置奇偶校验位，如果其中一个是Nan，可以通过奇偶校验位来验证，

  5:CPU执行指令的顺序：取指->译码->执行->访存->写回->更新PC，整个流程的简述就是，先从PC中取指令地址，取出指令代码和指令功能，然后在译码阶段读入操作数，执行阶段进行算数或逻辑运算，在访存阶段将数据写回内存最后更新PC的值。

  6:处理器体系架构，书中主要是介绍了流水线架构的基本原理和对应的实例Y86-64，对于Y86-64就不多记，流水线架构的原理本身也并不困难，可以分为五个阶段，即取指译码执行访存写回，根据这个顺序可能会出现数据冲突或控制冲突，即data hazard or control hazard。对应的解决方案从简单到复杂一共有三个阶段，第一个阶段介绍了采用简单的nop指令以解决冲突，第二个阶段采用了气泡(bubble)和nop指令结合的方式来解决冲突，比如nop nop bubble这种指令顺序。第三个阶段则是使用反馈的方式解决冲突，冲突的本身必定是某种访问冲突，无论是数据还是对控制，其实都是某块地址的访问冲突，反馈的方式是采用旁路监听的方式增加实际的针脚来将某些状态或者数据返回到下一条指令，比如赋值+运算的这种指令组合，在执行赋值时通过反馈将部分数据提前写到下一条指令的某个位置。一开始会写到下一条的执行阶段的位置，后来还是写入到下一条指令的译码阶段。

  7：CPE：每元素的周期数，因此优化CPU的性能可以用CPE来度量，每元素所占的周期数越少，说明性能越高。

  8: 循环展开：循环展开是一种对累乘的优化方法，这个方法的思想是：对于2X1的循环展开来讲，我们实际上会采用2次循环，一次循环完成步长为2的循环，另一次完成剩余次数的循环，即for(i=0;i<length-1;i+=2);for(;i<length;i++); 这种方式来实现累乘，循环1的最大index保证为length-1的原因是因为，对于任意元素来讲会存在i+2<length-1

  9：通过使用指令集并行优化代码的几种方式，包括循环展开，创建多个累积变量，项重新结合等方式。通过计算CPE，每元素周期数可以衡量执行性能。

  10:RAS和CAS共享相同的DRAM地址引脚，这意味着地址引脚并不会有ras*cas个，而是选取其中最大的，或者本身两个可能就相等（容量最大）。比如从设计的角度讲，5x3的地址和5x5的地址都需要5根地址引脚，那么肯定不会采用5x3。读取时，先传入行地址，然后取出这一整行到内部行缓冲区，然后传入列地址，从内部行缓冲区取出对应列。这个设计有效地减少了地址引脚的数量，但是也增加了访问的时间，影响了访问效率。

  11:关于DRAM的增强和发展，
  <table>
  <tr>
  <td>模式</td>
  <td>实现方式和特点</td>
  </tr>
  <tr><td>快页模式(FPM DRAM)</td><td>这个模式对传统DRAM的增强点在于取出的行缓冲区不会被丢弃，会是一连串从此行读取的指令，相当于行地址只取一次而不是多次。</td></tr>
  <tr><td>扩展数据输出(EDO DRAM)</td><td>快页模式的一种优化，允许CAS的信号在时间上更为紧凑</td></tr>
  <tr><td>同步DRAM(S DRAM)</td><td>SDRAM用与驱动控制器相同的外部时钟信号的上升沿来代替许多的控制信号，以此来达到比异步存储器更高的效率</td></tr>
  <tr><td>双倍速率同步DRAM(DDR SDRAM)</td><td>DDR SDRAM是对SDRAM的一个增强，通过使用两个时钟沿作为控制信号从而使DRAM的速度翻倍，</td></tr>
  <tr><td>视频RAM(V RAM)</td><td>用在图形系统的帧缓冲区中，比如可能OPENGL的帧缓冲区就用到了VRAM，有两点区别，1：VRAM的输出是通过依次对内部缓冲区的整个内容进行移位得到的。2：VRAM允许对内存并行的读和写。</td></tr>
  </table>

  12:固件：存储在ROM的程序，叫做ROM的固件。对于磁盘的发展来讲，从事实来看，内存和硬盘的技术在容量方面迭代很快，但是在访问时间只有2位数的倍数，容量上却有六个以上的数量级的提升。因此总结下来：增加密度从而降低成本比降低访问时间容易得多。

  13:局部性原理：一个编写良好的计算机程序，总是倾向于引用邻近的数据项或者是最近使用过的数据项。这种倾向性，就是局部性原理。局部性原理可以分为两类（根据定义即可看出，时间和空间）。

  14:高速缓存确定一个请求是否命中，然后抽取对应字的过程分为3步：组选择->行匹配->字抽取

  15:内存基础概念定义
  <table>
  <caption>基础参数</caption>
  <tr><td>参数</td><td>描述</td></tr>
  <tr><td>S=2^s</td><td>组数</td></tr>
  <tr><td>E</td><td>每个组的行数</td></tr>
  <tr><td>B=2^b</td><td>块大小，以字节为单位</td></tr>
  <tr><td>m=log2(M)</td><td>主存的物理地址位数</td></tr>
  </table>

  <table>
  <caption>衍生参数</caption>
  <tr><td>M=2^m</td><td>内存地址的最大数量</td></tr>
  <tr><td>s=log2(S)</td><td>组索引最大位数</td></tr>
  <tr><td>b=log2(B)</td><td>块偏移位数量</td></tr>
  <tr><td>t=m-(s+b)</td><td>标记位数量</td></tr>
  <tr><td>C=E*B*S</td><td>高速缓存大小，不包括有效位和标记位</td></tr>
  </table>

  16:内存的高速缓存策略策略有三种，直接映射高速缓存、组相联高速缓存策略、全相联高速缓存策略。直接映射高速缓存策略中每个组只有一行，因此组选择后直接验证标记位即可。组相联高速缓存策略中组会有多个行，因此组选择之后还需要遍历组中的行去判断标记位，并且因为存在多个行在同一个组，还会涉及到替换的问题，即高速缓存写入一个值应替换组中的哪一行？，因此会有行替换的策略，类似于页面置换策略。全相联高速缓存策略只有一个包含了所有行的组。组选择直接遍历所有行判断标记位即可。

  17：链接器的主要任务是：完成符号解析和重定位。符号解析主要是确保函数，全局变量，静态变量等含有声明和定义的，声明后必定存在对应的定义。重定位则是，给对应的符号分配对应的内存地址，相当于做了一次代码到地址的映射。

  18:CPP中重载的实现方式：编译器将每个唯一的方法和参数列表组合编码成一个对链接器来说唯一的名字。编码的过程叫重整，相反的过程叫恢复。

  19:引用静态链接库的方式，对于LINUX来讲，比如采用GCC编译的时候，gcc foo.c libx.a 这种格式是正确的，静态库文件放在后方，静态库文件如果之间互相有依赖关系，同之前的，被依赖的需要放在引用依赖文件的前面，比如libx.a liby.a， 如果y依赖x，那么x必须在y前面，简而言之，顺序引用。

  20:动态链接器和静态链接器的方式在某种意义上几乎是相反的，静态链接器是一开始会给对应的符号分配地址，因此需要符号的时候就去对应地址找即可。动态链接器是加载时，动态链接文件会去维护数据段开始的全局偏移量表，即GOT表，一个是顺序，一个是逆序。

  21:异常控制流，处理器检测到有事件发生时，会通过异常表这张跳转表跳转到对应的子程序中，当对应的异常处理程序完成之后，根据引起异常的事件的类型一般会有三种情况:1、处理程序将控制返回给触发程序。2、处理程序将控制返回给触发程序的下一条指令。3、处理程序终止被中断的程序。**但是需要谨记的是，异常不一定是错误**。

  22:题外话，SFINAE:Substitution Failture Is Not An Error

  23:CPU存在一个特殊的寄存器叫做异常表基址寄存器。在触发异常时会通过读取这个寄存器的地址跳转到异常处理程序。

  24：进程的经典定义就是一个执行中的程序的实例，每个程序都运行在进程的context（上下文）中，上下文是程序正确运行所需要的状态，状态包括内存中的程序的代码和数据，它的栈，通用目的寄存器的内容，程序计数器，环境变量，以及打开文件描述符的集合(the set of open file descriptors)。

  25：对于fork函数，可能是令人不解的，fork函数只会被调用一次但是却会返回两次，一次是父进程的，返回子进程的pid，一次是子进程的，返回0。返回值提供了一种区分父进程和子进程的方式。

  26：关于虚拟内存，页表，和物理内存的关系，我们基本上做的大部分操作，都是对虚拟内存，这个抽象成一个数组的结构来做的，当我们分配虚拟内存时，操作系统对页表的操作是将我们分配的虚拟内存进行一个映射，映射到对应的物理地址，这样给我们的假象就是我们被分配了一块物理地址。这个好处是，我们可以隔绝用户和物理地址的直接交互，因为我们采用了虚拟内存和页表，还有就是我们可以不必实时地和物理地址进行交互，我们对物理地址的所有操作都会写到页表。

  27:书上关于虚拟地址空间的好处写了以下4点：1、简化链接，独立的地址空间允许每个进程的内存映像使用相同的基本格式，而不关注实际的物理地址空间。这样的一致性极大地简化了链接器的设计和实现。2、简化加载，虚拟内存容易向内存中加载可执行文件和共享对象文件。加载器并不从磁盘往内存复制任何数据，每个页面初次被引用时，要嘛是由指令调入，要嘛是由CPU调入，无论哪个，都是由虚拟内存系统完成而不是加载器。3、简化共享，独立地址空间为操作系统提供了一个管理用户进程和操作系统自身之间共享的一致机制，试想，如果虚拟内存机制不完成对数据到物理地址的映射，那么假设操作系统需要共享某些数据，这时候操作系统需要自己去物理内存寻址，并且这个物理地址可能会变（由于代码，而且多半是不连续的）,现在这一部分工作可以交给虚拟内存机制完成，操作系统可以更专注于共享的机制。4、简化内存分配，如果用户请求额外的堆空间，对于操作系统来说，原本需要分配一块连续的物理地址，但是由于页表的工作方式，操作系统现在没有必要分配连续的物理页面，页面可以随机的分散在物理页面中。

  28:虚拟内存作为内存保护的工具，与其说虚拟内存作为内存保护的工具，不如说虚拟内存作为控制内存的工具，这有点类似于mvc模型，model负责数据，这点相当于物理页表，control负责控制，这点即是虚拟内存和页表的工作机制，view负责展示，即用户所需要的内存访问。

  29:页面命中完全是由硬件来处理的，与之不同的是，处理缺页要求硬件和操作系统协同处理。

  30:共享对象不需要写时复制，因为只有一份，但是私有对象则不同，举例来说，printf在各个代码中作为私有对象引用，应该在每个代码中都有一份，但是并不是一开始就会往所有代码中复制很多份。第一份创建->第二份创建->标记两份都为私有->第一份第二份任意一份对私有对象进行访问->进行写时复制。

  31：现在我们可以讨论一个问题，即分配产生的空间碎片，在WIN系统里这个非常常见，文件浏览器里会有对应磁盘分区的碎片整理功能，从上层来看已经给出了整理工具，那么我们从原理来看碎片，是如何产生的，我们又该如何来解决碎片。碎片分为两类，一类是以前的请求模式和分配器的实现方式产生的内部碎片(internal fragmentation),这一类碎片是由于存在已分配块比有效载荷大而发生。还有一类是外部碎片(external fragmentation)，外部碎片是由于空闲内存合起来能够满足一个分配请求，但是任意一个空闲内存都不能单独满足这个分配请求而导致的。由于外部碎片一直都有可能存在，我们必须有解决外部碎片的办法，因此我们需要了解关于分配器的数据结构。

  32:分配器数据结构-隐式空闲链表:整个结构由头部+有效载荷+填充构成。填充为可选项，根据对齐或其他要求确定有无，头部由块大小和是否分配的标记位构成。

  33:显式空闲链表，显式空闲链表和隐式空闲链表最大的区别就是，显式空闲链表利用结构可以仅遍历空闲块而不遍历分配块。显式空闲链表的结构是头部+prev+next+有效载荷+填充。

  34:一个使用单向空闲块链表的分配器需要与空闲块数量呈线性关系的时间来分配,我们可以采用分离存储的方式，即维护多个空闲链表，其中每个链表有大致相等的大小。分离存储的方法书上写了2种：简单分离存储和分离适配。简单分离存储：查找对应的空闲链表，直接取一块分配，优点很多：分配和释放都是很快的常数时间操作，每个块只有很小的内存开销（额外的，用于结构的）,大小相同也导致不需要头部和脚部，但是缺点在于很容易产生内部碎片。  分离适配：分离适配和前者的区别在于找到首次适配的块后，分配对应空间出去，也会切割剩下部分，去插入到适当的空闲链表中。  伙伴系统：伙伴系统是基于分离适配的改良，要求所有的块和空闲链表都是2的幂次大小，这样分配一个2^k块时，我们通过首次适配找到相等或稍大的块，相等的块则任务完成，稍大的则进行二分（因为规定都是2的幂次，因此二分必定有结果）.剩下的半块插入对应空闲链表，这种方式的优点在于快速查找和快速合并，这个基于被二分的块其实地址只有一位不同。

  35:open函数总是返回最低的未被打开的描述符，在unix系统中，stdin（描述符0）,stdout（描述符1）,stderr（描述符2）。

  36:linux内核用三个相关的数据结构来表示打开的文件:描述符表(decriptor table)：每个进程有独立的描述符表，表项是由进程打开的文件的描述符索引的，每个打开的描述符表项指向文件表中的一个表项。文件表(file table)：文件表由所有的进程共享，每个文件表的表项的构成包括当前文件位置、引用计数、一个指向v-node表表项的指针。**关闭一个描述符会减少对应表项的引用计数，内核不会删除这个文件表表项，直至这个表项的引用技术为0。**(智能指针的思想) v-node表:所有进程同样共享v-node表，每个表项包含stat结构的大多数信息。(stat结构是指stat函数所使用的一个struct，包含了文件的大多数用得上的信息，包括但不限于文件名，路径，各种时间等)

  37:对于一个文本文件中内容为"foo"的。我们调用两次open读取第一个char，得到的都是f，而不会是f o的序列。而对于父子进程来讲，父进程和子进程有相同的描述符表，以及共享的文件表,因此子进程读了一个f字符后，父进程再读一个字符将会是o，即是fo序列而不是ff序列。

  38：标准IO流的限制和套接字的限制冲突导致网络输入输出冲突。现象如下：1-跟在输出函数之后的输入函数。即cout;cin,如果中间没有插入对 fflush、fseek、fsetpos或者rewind的调用，一个输入函数不能跟在一个输出函数之后，fflush函数清空与流相关的缓冲区，后三个使用lseek这个unix io来重置当前文件位置。(对于cpp，合理解释了为什么会有cout<<value<<endl;这个习惯，因为如果是网络接口的编程，不通过endl刷新缓冲区可能是会引起冲突的。) 2-跟在输入函数后的输出函数，如果中间没有插入对fseek、fsetpos、rewind的调用，一个输出函数不能跟在一个输入函数之后，除非该函数遇到了一个EOF。 限制1可以通过刷新缓冲区来满足，但是对于第二个限制的唯一办法是，对同一个套接字描述符打开2个流，分别用来输入和输出。所付出的额外代价仅仅是需要close2次。

  39:对于网络相关的接口，IP地址被定义为一个32位的unsigned int，在网络上的传输是通过大端法，即使使用的intel系的小端机器。因此unix提供了一些函数以供小端机器在使用网络接口时对IP地址的转换。例如htonl htons等。

  40:监听描述符和已连接描述符，很好理解，按照面向对象的思想来看，监听描述符是将服务端作为一个整体，属于服务端的成员变量，因此生命周期由服务端关闭决定，而已连接描述符的生命周期则和连接的生命周期一致，是属于端到端的一个连接的，一旦连接断开，则销毁。很容易想到的一个场景就是，服务器可以有一个监听描述符，但是可以有多个已连接描述符，因为服务器和客户端肯定需要有一对多的场景。

  41:EOF文件结束符，EOF无论是在磁盘文件还是将网络传输抽象成文件系统的情况下，都不是一个具体的ascii字符，或者一串。EOF准确说应该是一个判断条件，对于磁盘文件来讲，当前文件位置超出文件大小时，会发生EOF，对于网络传输，一个进程关闭连接它的那一端时会发生EOF。

  42：web服务器以两种不同的方式为客户端提供内容：1-取一个磁盘文件，并将它的内容返回给客户端，磁盘文件叫做静态内容，返回文件内容给客户端的过程叫做服务静态内容。2-执行一个可执行文件，执行可执行文件产生的输出叫做动态内容，返回输出给客户端的过程叫做服务动态内容，此处的可执行文件不一定是指EXE。

  43：现代计算机系统提供了三种构造并发程序的方式：1-进程，每个逻辑控制流都是一个进程，由内核调度和维护。想要和其他流通信，控制流必须使用某种显式的进程通信机制。2-IO多路复用，应用程序在一个进程的上下文中显式地调度它们自己地逻辑流，逻辑流被模型化为状态机，数据到达文件描述符之后，主程序显式地从一个状态切换到另一个状态。3-线程，线程是运行在单一进程中的逻辑流，由内核进行调度。（对于线程间的通信，线程间共享进程的地址空间，但是对于每一个线程的修改，如果需要做同步处理，就会碰到常见的CPP的线程通信的函数。比较的麻烦）

  44：IO多路复用，书中举的例子是select函数，即网络编程中通过select轮询处理IO。就是所谓的IO多路复用。

  45：信号量不变性-P和V的定义拥有原子性，保证了一个正在运行的程序绝对不会出现这样一种状态，正确初始化了的信号量有一个负值。

  46：信号量分为二元信号量和计数信号量，二元信号量的初值为1，因此只会出现0和1两个值。计数信号量一般是一组可用资源的信号量。

  47：并行编程同步开销巨大，要尽可能避免同步，如果不可避免，必须用尽可能多的有用计算弥补这个开销。

  48：加速比相关的概念：S<sub>p</sub>=T<sub>1</sub>/T<sub>p</sub>,其中p是核数，T<sub>K</sub>是在K个核的运行时间，T1是顺序执行版本的执行时间时，这个被叫做**绝对加速比**，T1是并行版本在一个核上的执行时间时，这个叫做**相对加速比**

  49：效率的概念：E<sub>p</sub>=S<sub>p</sub>/p=T<sub>1</sub>/pT<sub>p</sub>;

  50：定义出的四个不相交的线程不安全的函数类：1-不保护共享变量的函数。（可通过信号量解决，不需要修改调用程序，会减慢程序运行时间。）2-保持跨越多个调用的状态的函数-伪随机数生成器，简化下来就是，n=n*5;return n;这个n不一定是想要的值。3-返回指向静态变量的指针的函数，多线程调用时可能出现指针被其他线程覆盖的错误情况。（处理方式：1-重写，消除共享数据。2-加锁复制，在每一个调用位置，对互斥锁进行加锁，调用线程不安全函数，将函数返回结果复制到一个私有的内存位置，然后解锁并返回。）4-调用线程不安全函数。

  51：显式可重入的概念：如果函数的所有参数都是按值传递的，并且所有的数据的引用都是自己的栈的变量，那么函数就是显式可重入的。

  52：隐式可重入的概念：隐式可重入是将显示可重入的标准放宽，允许按地址传递参数。那么数据的引用就存在部分不是自己的栈的，但是调用线程需要小心地传递**非共享数据**的指针。

  53：互斥操作避免死锁的方式：给定所有互斥操作一个全序，如果每个线程都是以一个顺序获取互斥锁并以相反的顺序释放，那么程序不会产生死锁。



